# Snake-AI com PyTorch

Este projeto consiste em uma Intelig√™ncia Artificial, baseada em Aprendizagem por Refor√ßo (*Deep Q-Learning*), que aprende a jogar o cl√°ssico jogo da Cobra do zero. O agente interage com o ambiente do jogo, toma decis√µes e recebe recompensas ou puni√ß√µes com base em suas a√ß√µes, melhorando seu desempenho ao longo do tempo.

## ‚ú® Funcionalidades

  * **Agente Aut√¥nomo**: O agente aprende e joga de forma completamente independente.
  * **Rede Neural (DQN)**: A l√≥gica de decis√£o √© baseada em uma rede neural constru√≠da com PyTorch.
  * **Aprendizagem por Refor√ßo**: O agente aprende atrav√©s do m√©todo de Q-Learning, otimizando suas a√ß√µes para maximizar a pontua√ß√£o futura.
  * **Visualiza√ß√£o em Tempo Real**: Um gr√°fico gerado com Matplotlib exibe a pontua√ß√£o e a pontua√ß√£o m√©dia em tempo real durante o treinamento.
  * **Ambiente com Pygame**: O ambiente do jogo √© totalmente interativo e foi constru√≠do com a biblioteca Pygame.
  * **Salvamento de Modelo**: O melhor modelo √© salvo automaticamente na pasta `model/` sempre que um novo recorde de pontua√ß√£o √© atingido.

## üõ†Ô∏è Tecnologias Utilizadas

  * **Python 3.8+**
  * **PyTorch**: Para a cria√ß√£o da rede neural e o processo de treinamento.
  * **Pygame**: Para a cria√ß√£o do ambiente do jogo.
  * **NumPy**: Para manipula√ß√£o de arrays e estados.
  * **Matplotlib**: Para a plotagem do gr√°fico de desempenho.

## ‚öôÔ∏è Instala√ß√£o e Configura√ß√£o

Para executar este projeto em sua m√°quina local, siga os passos abaixo.

**1. Clone o reposit√≥rio:**

```bash
git clone <URL_DO_SEU_REPOSITORIO>
cd <NOME_DA_PASTA_DO_PROJETO>
```

**2. Crie um ambiente virtual (recomendado):**

```bash
python -m venv venv
```

  * No Windows, ative com:
    ```bash
    .\venv\Scripts\activate
    ```

**3. Instale as depend√™ncias:**
Crie um arquivo chamado `requirements.txt` na raiz do projeto com o seguinte conte√∫do:

```txt
torch
pygame
numpy
matplotlib
```

Em seguida, instale as bibliotecas com o comando:

```bash
pip install -r requirements.txt
```

## Como Usar

Com o ambiente configurado e as depend√™ncias instaladas, inicie o treinamento do agente executando o script `agent.py`:

```bash
python agent.py
```

Duas janelas ser√£o abertas:

1.  A janela do jogo da cobra, onde voc√™ ver√° a IA jogando.
2.  A janela do gr√°fico, que mostrar√° o progresso do treinamento em tempo real.

O console exibir√° as informa√ß√µes de cada partida, incluindo a pontua√ß√£o e o recorde atual. Para parar o treinamento, basta fechar a janela do jogo.

## üìÅ Estrutura dos Arquivos

```
.
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îî‚îÄ‚îÄ model.pth      # Modelo salvo automaticamente
‚îú‚îÄ‚îÄ agent.py           # Script principal com o loop de treinamento
‚îú‚îÄ‚îÄ model.py           # Define a rede neural (DQN) e o QTrainer
‚îú‚îÄ‚îÄ snake_game_env.py  # Define o ambiente do jogo com Pygame
‚îú‚îÄ‚îÄ plot.py            # Utilit√°rio para plotar o gr√°fico em tempo real
‚îî‚îÄ‚îÄ README.md          # Este arquivo
```

## Como Funciona

O agente aprende seguindo o ciclo do Aprendizado por Refor√ßo:

1.  **Estado (State)**: O agente recebe um estado que descreve a situa√ß√£o atual do jogo. Este estado inclui informa√ß√µes como:

      * Perigo imediato (em frente, √† direita, √† esquerda).
      * Dire√ß√£o atual da cobra.
      * Posi√ß√£o da comida em rela√ß√£o √† cabe√ßa da cobra.
      * Dist√¢ncia at√© as paredes.
      * Presen√ßa de partes do corpo da cobra em dire√ß√µes adjacentes.

2.  **A√ß√£o (Action)**: Com base no estado, a rede neural decide qual a melhor a√ß√£o a ser tomada: `[seguir reto, virar √† direita, virar √† esquerda]`.

3.  **Recompensa (Reward)**: Ap√≥s executar a a√ß√£o, o agente recebe uma recompensa:

      * `+10` por comer a comida.
      * `-100` por morrer (colidir com a parede ou com o pr√≥prio corpo).
      * `-0.01` por cada passo dado (para incentivar a efici√™ncia).

O algoritmo **Deep Q-Learning** treina a rede neural para prever a a√ß√£o que levar√° √† maior recompensa acumulada ao longo do tempo. O agente utiliza uma "mem√≥ria de repeti√ß√£o" para reaprender com experi√™ncias passadas e uma estrat√©gia *epsilon-greedy* para balancear entre explorar novas jogadas e utilizar as que j√° sabe que s√£o boas.
